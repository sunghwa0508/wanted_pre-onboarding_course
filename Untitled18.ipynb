{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzE63ScZ00QD9OnObiPoGw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunghwa0508/wanted_pre-onboarding_course/blob/main/Untitled18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원티드 프리온보딩 코스"
      ],
      "metadata": {
        "id": "RKqtscbRCRHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ScBzKf1KEHAb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from collections import Counter\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    for word in sequences:\n",
        "      #소문자변환 & 정규화\n",
        "      sentence  = str.lower(word)\n",
        "      sentence = re.sub(r\"[^a-z0-9 \\n ]\", \"\", sentence )\n",
        "      # white space단위로 토큰화\n",
        "      result.append(sentence.split())\n",
        "      \n",
        "    return result\n",
        "\n",
        "\n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    self.fit_checker = True\n",
        "    token = self.preprocessing(sequences)\n",
        "    words = sum(token, [])\n",
        "\n",
        "    #빈도수\n",
        "    vocab = Counter(words)\n",
        "\n",
        "    #정렬\n",
        "    vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True)\n",
        "\n",
        "    #인덱스부여\n",
        "    word_to_index = {}\n",
        "    i = 0\n",
        "    for (word, frequency) in vocab_sorted:\n",
        "      if frequency >0:\n",
        "        i += 1\n",
        "        word_to_index[word] = i\n",
        "\n",
        "    self.word_dict.update(word_to_index)\n",
        "    return self.word_dict\n",
        "\n",
        "\n",
        "\n",
        "  def transform(self, sequences):\n",
        "    tokens = self.preprocessing(sequences)\n",
        "\n",
        "    if self.fit_checker:\n",
        "      words = self.fit(sequences)\n",
        "      #정수인코딩\n",
        "      result=[]\n",
        "      for s in tokens:\n",
        "        temp = []\n",
        "        for w in s:\n",
        "          try:\n",
        "            temp.append(words[w])\n",
        "          except KeyError:\n",
        "            temp.append(words['oov'])\n",
        "        result.append(temp)\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AA = Tokenizer()\n",
        "AA.preprocessing(['I go to school.', 'I LIKE pizza!'])\n",
        "AA.fit(['I go to school.', 'I LIKE pizza!'])\n",
        "AA.transform(['I go to school.', 'I LIKE pizza!'])\n",
        "AA.fit_transform(['I go to school.', 'I LIKE pizza!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2OEYmHXErxV",
        "outputId": "1e7c290a-586c-4e1a-d355-c3921d1e2eba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from math import log\n",
        "\n",
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "\n",
        "    self.idf = []\n",
        "\n",
        "    N = len(sequences)\n",
        "\n",
        "    num_token = len(self.tokenizer.word_dict.values())\n",
        "    \n",
        "    for i in range(num_token):\n",
        "      num = 0\n",
        "      for j in tokenized:\n",
        "        if i in j:\n",
        "            num += 1\n",
        "\n",
        "      self.idf.append(log(N/(num+1)))\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "      if self.fit_checker:\n",
        "          tokenized = self.tokenizer.transform(sequences)\n",
        "\n",
        "          num_token = len(self.tokenizer.word_dict.values())\n",
        "\n",
        "          self.tfidf_matrix = []\n",
        "\n",
        "          for i in tokenized:\n",
        "            tf = []\n",
        "            for j in range(num_token):\n",
        "              tf.append(i.count(j))\n",
        "\n",
        "            a = [x*y for x,y in zip(tf, self.idf)]\n",
        "\n",
        "            self.tfidf_matrix.append(a)\n",
        "\n",
        "          \n",
        "          return self.tfidf_matrix\n",
        "      else:\n",
        "          raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "          \n",
        "  def fit_transform(self, sequences):\n",
        "      self.fit(sequences)\n",
        "      \n",
        "      return self.transform(sequences)"
      ],
      "metadata": {
        "id": "uqoHZABTEug7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BB = TfidfVectorizer(AA)\n",
        "BB.fit(['I go to school.', 'I LIKE pizza!'])\n",
        "BB.transform(['I go to school.', 'I LIKE pizza!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1KsBp7TlEyY",
        "outputId": "7ee14f8b-2591-4bbd-b721-a9bde4007081"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}